# ETL Project

Pull current-conditions data from **OpenWeatherMap**, tidy it with **pandas**, and load it into **PostgreSQL** on an hourly schedule.  
The pipeline is orchestrated by **Apache Airflow** and ships with a one-command Docker Compose stack for local runs.

---

## Prerequisites

| Tool | Why you need it | Minimum version |
|------|-----------------|-----------------|
| **Docker Engine** | Runs the containerised stack (Airflow, Postgres, pgAdmin). | 20.x |
| **Docker Compose** | Orchestrates the multi-container setup defined in `docker-compose.yaml`. | v2 plugin (shipped with Docker Desktop ≥ 4.x) |
| **Git** | To clone this repository. | Any modern version |

> **Already have Docker & Compose?** Great—no re-install needed.  
> If you’re new to Docker, download **Docker Desktop** (Win/Mac) or follow the [Docker Engine install guide](https://docs.docker.com/engine/install/) for Linux. Airflow and Postgres are pulled as images when you run `docker compose up`, so nothing else needs local installation.


## Installation

1. Obtain an API Key from [OpenWeather](https://openweathermap.org/)

2. Clone the repository and cd into root folder
```bash
git clone https://github.com/brian-vu-nguyen/weather-etl.git
cd weather-etl
```

3. Copy `docker-compose.yaml.example`
> Remove `.example`

*OR* start from scratch:
``` bash
curl -LfO 'https://airflow.apache.org/docs/apache-airflow/3.0.2/docker-compose.yaml' 
```


## Configure
1. Provide your API Key
```bash
echo "API_KEY=<your-openweather-api-key>" > .env          # used by extract.py
```

2. Configure `docker-compose.yaml` & `.env` (credentials, usernames, passwords, ports, volumes)

3. Add files to `.gitignore` (create file if it doesn't exist)
```bash
echo -e ".env\ndocker-compose.yaml\n" >> .gitignore
```

4. Spin-up stack
```bash
docker compose up -d          # brings up airflow-{web, scheduler, worker}, postgres, pgadmin
docker compose ps             # view stack details
```

5. Log in to Airflow & Postgres GUI
```bash
open http://localhost:8080    # user: airflow / pw: airflow  (default)
open http://localhost:5432    # pg_default_email: <your-default-email> / pg_default_pw: <your-default-pw>
```

6. Add Airflow Variables & Connections 

7. Flip the “weather_etl_pipeline” DAG switch to **On**

---

## Docker Cheatsheet

Some common docker commands you may use:


Initialize Apache Airflow
```bash
docker compose up airflow-init
```


Start and run multi-container services (in the background) defined in `docker-compose.yaml`
```bash
docker compose up -d
```  


Stops and removes containers, networks, volumes, and images created by docker compose up
>Only containers and networks are removed by default
```bash
docker compose down
```  


Displays the status of containers defined in `docker-compose.yaml`
```bash
docker compose ps
```


Retrieve and view the logs generated by a container
```bash
docker logs
```